{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "\n",
    "# import albumentations as A\n",
    "# from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "#Trainer Imports\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "\n",
    "from matplotlib import pyplot as plt #for visualizing \n",
    "\n",
    "#From the local python files\n",
    "from datasets import DenoisingPairedDataset\n",
    "from trainer import DenoisingTrainer #What is going awn\n",
    "# from trainer import perceptual_loss, combined_loss, DenoisingTrainer\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Root directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = 'datasets/aquarium-data-cots/aquarium_pretrain'\n",
    "\n",
    "splits = ['train', 'test','valid']\n",
    "\n",
    "model_path = '/home/ubuntu/cs230_VIVEKA/saved_models'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Denoising Model Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture 1: basic model.\n",
    "consider adding: dropout, batch normalization layers, skip connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class DenoisingCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DenoisingCNN, self).__init__()\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),  # Downsample once\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2)  # Downsample again\n",
    "        )\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 3, kernel_size=3, padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture 1.5 (WIP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenoisingCNN_v2(nn.Module):\n",
    "    ''' Simple CNN architecture for the first iteration of Denoising model training.\n",
    "        simple encoder-decoder structure with dropout of\n",
    "        0.2 initially and 0.4 at the last layer of encoder (and no dropout for the final layer of the decoder).\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super(DenoisingCNN_v2, self).__init__()\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "            #nn.Dropout(0.2),\n",
    "\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "            #nn.Dropout(0.2),\n",
    "\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            #nn.Dropout(0.4)\n",
    "            \n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "        )\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            #nn.Dropout(0.2),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=2, stride=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            #nn.Dropout(0.2),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(32, 3, kernel_size=3, padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture 2\n",
    "find pre existing model that targets issues of the images, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenoisingUNet(nn.Module):\n",
    "    ''' Denoising CNN class with a U-Net architecture.\n",
    "        3 contractive layers (encoder) and 3 expansive layers (decode).\n",
    "        Final layer is Sigmoid to map pixel values to (0,1).\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super(DenoisingUNet, self).__init__()\n",
    "        # Encoder\n",
    "        self.enc_conv1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.enc_conv1a = nn.Conv2d(32, 32, kernel_size=3, padding=1)  # Additional layer\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "\n",
    "        self.enc_conv2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.enc_conv2a = nn.Conv2d(64, 64, kernel_size=3, padding=1)  # Additional layer\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "\n",
    "        self.enc_conv3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.enc_conv3a = nn.Conv2d(128, 128, kernel_size=3, padding=1)  # Additional layer\n",
    "        self.dropout3 = nn.Dropout(0.4)\n",
    "        \n",
    "        # Decoder\n",
    "        self.dec_conv1 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.dropout4 = nn.Dropout(0.2)\n",
    "\n",
    "        self.dec_conv2 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 32, kernel_size=2, stride=2),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.dropout5 = nn.Dropout(0.2)\n",
    "\n",
    "        self.final_conv = nn.Sequential(\n",
    "            nn.Conv2d(64, 3, kernel_size=3, padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        x1 = self.enc_conv1(x)\n",
    "        x1 = self.enc_conv1a(x1)  # Pass through additional layer\n",
    "        x1_pooled = self.pool1(x1)\n",
    "        x1_pooled = self.dropout1(x1_pooled)\n",
    "\n",
    "        x2 = self.enc_conv2(x1_pooled)\n",
    "        x2 = self.enc_conv2a(x2)  # Pass through additional layer\n",
    "        x2_pooled = self.pool2(x2)\n",
    "        x2_pooled = self.dropout2(x2_pooled)\n",
    "\n",
    "        x3 = self.enc_conv3(x2_pooled)\n",
    "        x3 = self.enc_conv3a(x3)  # Pass through additional layer\n",
    "        x3 = self.dropout3(x3)\n",
    "\n",
    "        # Decoder with Skip Connections\n",
    "        x4 = self.dec_conv1(x3)\n",
    "        x4 = self.dropout4(x4)\n",
    "        x4 = torch.cat((x4, x2), dim=1)  # Skip connection from encoder layer 2\n",
    "\n",
    "        x5 = self.dec_conv2(x4)\n",
    "        x5 = self.dropout5(x5)\n",
    "        x5 = torch.cat((x5, x1), dim=1)  # Skip connection from encoder layer 1\n",
    "\n",
    "        out = self.final_conv(x5)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture 3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImprovedDenoisingUNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ImprovedDenoisingUNet, self).__init__()\n",
    "        # Encoder\n",
    "        self.enc_conv1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "\n",
    "        self.enc_conv2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "\n",
    "        self.enc_conv3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        # Decoder\n",
    "        self.dec_conv1 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.dec_conv2 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(64 + 64, 32, kernel_size=2, stride=2),  # Skip connection\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.final_conv = nn.Conv2d(32 + 32, 3, kernel_size=3, padding=1)  # Skip connection\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        x1 = self.enc_conv1(x)\n",
    "        x1_pooled = self.pool1(x1)\n",
    "\n",
    "        x2 = self.enc_conv2(x1_pooled)\n",
    "        x2_pooled = self.pool2(x2)\n",
    "\n",
    "        x3 = self.enc_conv3(x2_pooled)\n",
    "\n",
    "        # Decoder with Skip Connections\n",
    "        x4 = self.dec_conv1(x3)\n",
    "        x4 = torch.cat((x4, x2), dim=1)  # Skip connection\n",
    "\n",
    "        x5 = self.dec_conv2(x4)\n",
    "        x5 = torch.cat((x5, x1), dim=1)  # Skip connection\n",
    "\n",
    "        out = self.final_conv(x5)\n",
    "\n",
    "        return torch.sigmoid(out)  # Map to [0, 1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture 3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualUNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResidualUNet, self).__init__()\n",
    "        # Encoder\n",
    "        self.enc_conv1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.enc_conv2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "\n",
    "        # Bottleneck\n",
    "        self.bottleneck = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        # Decoder\n",
    "        self.dec_conv1 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.dec_conv2 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=2, stride=2),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.final_conv = nn.Conv2d(32, 3, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        x1 = self.enc_conv1(x)\n",
    "        x1_pooled = self.pool(x1)\n",
    "\n",
    "        x2 = self.enc_conv2(x1_pooled)\n",
    "        x2_pooled = self.pool(x2)\n",
    "\n",
    "        # Bottleneck\n",
    "        bottleneck = self.bottleneck(x2_pooled)\n",
    "\n",
    "        # Decoder\n",
    "        x3 = self.dec_conv1(bottleneck) + x2  # Residual connection\n",
    "        x4 = self.dec_conv2(x3) + x1  # Residual connection\n",
    "\n",
    "        out = torch.sigmoid(self.final_conv(x4))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Denoising Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "# from skimage.metrics import structural_similarity as ssim\n",
    "# import numpy as np\n",
    "\n",
    "# class DenoisingTrainer:\n",
    "#     def __init__(self, model, device, criterion, optimizer, save_path='best_model.pth'):\n",
    "#         self.model = model\n",
    "#         self.device = device\n",
    "#         self.criterion = criterion\n",
    "#         self.optimizer = optimizer\n",
    "#         self.save_path = save_path\n",
    "\n",
    "#         self.train_losses = []\n",
    "#         self.val_losses = []\n",
    "#         self.best_val_loss = float('inf')\n",
    "\n",
    "#     def train(self, train_loader, valid_loader, num_epochs=10):\n",
    "#         for epoch in range(num_epochs):\n",
    "#             self.model.train()\n",
    "#             running_train_loss = 0.0\n",
    "#             total_train_samples = 0\n",
    "\n",
    "#             for noisy_images, clean_images in train_loader:\n",
    "#                 noisy_images = noisy_images.to(self.device)\n",
    "#                 clean_images = clean_images.to(self.device)\n",
    "\n",
    "#                 self.optimizer.zero_grad()\n",
    "#                 outputs = self.model(noisy_images)\n",
    "#                 loss = self.criterion(outputs, clean_images)\n",
    "#                 loss.backward()\n",
    "#                 self.optimizer.step()\n",
    "\n",
    "#                 running_train_loss += loss.item() * noisy_images.size(0)\n",
    "#                 total_train_samples += noisy_images.size(0)\n",
    "\n",
    "#             epoch_train_loss = running_train_loss / total_train_samples\n",
    "#             self.train_losses.append(epoch_train_loss)\n",
    "#             print(f'Epoch {epoch+1}/{num_epochs}, Training Loss: {epoch_train_loss:.4f}')\n",
    "\n",
    "#             self.model.eval()\n",
    "#             running_val_loss = 0.0\n",
    "#             total_val_samples = 0\n",
    "\n",
    "#             with torch.no_grad():\n",
    "#                 for noisy_images, clean_images in valid_loader:\n",
    "#                     noisy_images = noisy_images.to(self.device)\n",
    "#                     clean_images = clean_images.to(self.device)\n",
    "\n",
    "#                     outputs = self.model(noisy_images)\n",
    "#                     loss = self.criterion(outputs, clean_images)\n",
    "\n",
    "#                     running_val_loss += loss.item() * noisy_images.size(0)\n",
    "#                     total_val_samples += noisy_images.size(0)\n",
    "\n",
    "#             epoch_val_loss = running_val_loss / total_val_samples\n",
    "#             self.val_losses.append(epoch_val_loss)\n",
    "#             print(f'Epoch {epoch+1}/{num_epochs}, Validation Loss: {epoch_val_loss:.4f}')\n",
    "\n",
    "#             if epoch_val_loss < self.best_val_loss:\n",
    "#                 self.best_val_loss = epoch_val_loss\n",
    "#                 torch.save(self.model.state_dict(), self.save_path)\n",
    "#                 print(f\"Model saved with validation loss: {epoch_val_loss:.4f}\")\n",
    "\n",
    "#         #Plotting train and validation loss per epoch at the end of train() function\n",
    "#         plt.figure(figsize=(10, 6))\n",
    "#         plt.plot(range(1, len(self.train_losses) + 1), self.train_losses, label='Training Loss')\n",
    "#         plt.plot(range(1, len(self.val_losses) + 1), self.val_losses, label='Validation Loss')\n",
    "#         plt.xlabel('Epochs')\n",
    "#         plt.ylabel('Loss')\n",
    "#         plt.title('Training and Validation Loss per Epoch')\n",
    "#         plt.legend()\n",
    "#         plt.grid(True)\n",
    "#         plt.show()\n",
    "\n",
    "#     def evaluate(self, data_loader):\n",
    "#         ''' Evaluates MSE loss, pSNR, and SSIM of the model output to the ground truth labels\n",
    "#             using skimage.metrics for the latter two metrics.\n",
    "#         '''\n",
    "#         self.model.eval()\n",
    "#         running_loss = 0.0\n",
    "#         total_samples = 0\n",
    "#         total_psnr = 0.0\n",
    "#         total_ssim = 0.0\n",
    "\n",
    "#         with torch.no_grad():\n",
    "#             for noisy_images, clean_images in data_loader:\n",
    "#                 noisy_images = noisy_images.to(self.device)\n",
    "#                 clean_images = clean_images.to(self.device)\n",
    "\n",
    "#                 outputs = self.model(noisy_images)\n",
    "#                 loss = self.criterion(outputs, clean_images)\n",
    "\n",
    "#                 running_loss += loss.item() * noisy_images.size(0)\n",
    "#                 total_samples += noisy_images.size(0)\n",
    "\n",
    "#                 # METRICS pSNR, SSIM ADDED HERE\n",
    "#                 outputs_np = outputs.cpu().numpy().transpose(0, 2, 3, 1)\n",
    "#                 clean_images_np = clean_images.cpu().numpy().transpose(0, 2, 3, 1)\n",
    "                \n",
    "#                 # debugging ||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
    "#                 # print(\"Output shape:\", outputs_np.shape)\n",
    "#                 # print(\"Clean image shape:\", clean_images_np.shape)\n",
    "#                 # end debugging ||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
    "\n",
    "#                 for o, c in zip(outputs_np, clean_images_np):\n",
    "#                     total_psnr += psnr(c, o, data_range=1.0)\n",
    "#                     total_ssim += ssim(c, o, data_range=1.0, win_size=3, channel_axis=-1)\n",
    "\n",
    "#         avg_loss = running_loss / total_samples\n",
    "#         avg_psnr = total_psnr / total_samples\n",
    "#         avg_ssim = total_ssim / total_samples\n",
    "\n",
    "#         print(f\"Avg Loss: {avg_loss:.4f}, Avg PSNR: {avg_psnr:.4f}, Avg SSIM: {avg_ssim:.4f}\")\n",
    "#         return avg_loss, avg_psnr, avg_ssim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main model training pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### create datasets, dataloaders, model, trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams = ['batch_size', 'lr', 'num_epochs', 'dropout_rate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transforms\n",
    "normalization_mean = normalization_std = [0.5, 0.5, 0.5]\n",
    "\n",
    "transform_normalize = T.Compose([T.Resize((224, 224)),  #resize\n",
    "                        T.ToTensor(),                   # to tensor\n",
    "                        T.Normalize(mean=normalization_mean, std=normalization_std)]) #normalize\n",
    "\n",
    "transform_regular = T.ToTensor()\n",
    "\n",
    "transform_resize = T.Compose( [ T.Resize((224,224)) , T.ToTensor() ] ) #IMPORTANT: size is taken from datasets.py transform in order to match "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = DenoisingPairedDataset(root_dir=root_dir, split='train', transform=transform_resize)\n",
    "valid_data = DenoisingPairedDataset(root_dir=root_dir, split='valid', transform=transform_resize)\n",
    "test_data = DenoisingPairedDataset(root_dir=root_dir, split='test', transform=transform_resize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_data, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create Model, device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define models\n",
    "model_simple = DenoisingCNN()\n",
    "# model_unet = DenoisingUNet()\n",
    "\n",
    "# model_simple_v2 = DenoisingCNN_v2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerceptualLoss(nn.Module):\n",
    "    def __init__(self, vgg, layers=None):\n",
    "        super(PerceptualLoss, self).__init__()\n",
    "        \n",
    "        # Use specific layers from the VGG model (e.g., up to 'conv_4')\n",
    "        if layers is None:\n",
    "            layers = ['0', '5', '10', '19']  # You can choose different layers\n",
    "        self.layers = layers\n",
    "        \n",
    "        # Extract the layers from VGG model\n",
    "        self.vgg_layers = nn.ModuleList([vgg[int(i)] for i in layers])\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        # Extract features at selected layers from both the input images\n",
    "        x_features = self.extract_features(x)\n",
    "        y_features = self.extract_features(y)\n",
    "        \n",
    "        # Compute the perceptual loss (MSE loss between feature maps)\n",
    "        loss = 0.0\n",
    "        for x_feat, y_feat in zip(x_features, y_features):\n",
    "            loss += F.mse_loss(x_feat, y_feat)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def extract_features(self, x):\n",
    "        features = []\n",
    "        for i, layer in enumerate(self.vgg_layers):\n",
    "            x = layer(x)\n",
    "            if str(i) in self.layers:\n",
    "                features.append(x)\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DenoisingCNN(\n",
       "  (encoder): Sequential(\n",
       "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "  )\n",
       "  (decoder): Sequential(\n",
       "    (0): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): Conv2d(32, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = model_simple\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "\n",
    "# Load pre-trained VGG-16 model\n",
    "vgg = models.vgg16(pretrained=True).features\n",
    "\n",
    "# Set to evaluation mode (important)\n",
    "vgg.eval()\n",
    "\n",
    "# Move the model to the same device as your neural network\n",
    "vgg = vgg.to(device)\n",
    "\n",
    "# Freeze VGG parameters so they aren't updated during training\n",
    "for param in vgg.parameters():\n",
    "    param.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "# Assuming 'model' is your denoising model, and 'train_loader' is your DataLoader\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Perceptual Loss function\n",
    "perceptual_criterion = PerceptualLoss(vgg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = model_simple\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# criterion = nn.MSELoss()\n",
    "perceptual_criterion = PerceptualLoss(vgg)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = DenoisingTrainer(model, device, perceptual_criterion, optimizer, save_path=\"saved_models/best_denoising_model_unet.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 21.95 GiB of which 17.88 MiB is free. Process 5819 has 8.65 GiB memory in use. Process 6592 has 186.00 MiB memory in use. Including non-PyTorch memory, this process has 13.08 GiB memory in use. Of the allocated memory 12.86 GiB is allocated by PyTorch, and 9.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[80], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/cs230_VIVEKA/trainer.py:247\u001b[0m, in \u001b[0;36mDenoisingTrainer.train\u001b[0;34m(self, train_loader, valid_loader, num_epochs)\u001b[0m\n\u001b[1;32m    244\u001b[0m total_train_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m noisy_images, clean_images \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m--> 247\u001b[0m     noisy_images \u001b[38;5;241m=\u001b[39m \u001b[43mnoisy_images\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    248\u001b[0m     clean_images \u001b[38;5;241m=\u001b[39m clean_images\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    250\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 21.95 GiB of which 17.88 MiB is free. Process 5819 has 8.65 GiB memory in use. Process 6592 has 186.00 MiB memory in use. Including non-PyTorch memory, this process has 13.08 GiB memory in use. Of the allocated memory 12.86 GiB is allocated by PyTorch, and 9.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "trainer.train(train_loader, valid_loader, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss, val_psnr, val_ssim = trainer.evaluate(valid_loader)\n",
    "#print(f\"Validation - Loss: {val_loss:.4f}, PSNR: {val_psnr:.4f}, SSIM: {val_ssim:.4f}\")\n",
    "\n",
    "test_loss, test_psnr, test_ssim = trainer.evaluate(test_loader)\n",
    "#print(f\"Test - Loss: {test_loss:.4f}, PSNR: {test_psnr:.4f}, SSIM: {test_ssim:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# visualize dataset, model outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### visualize X,Y pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unnormalize(img_tensor, mean, std):\n",
    "    \"\"\"\n",
    "    Unnormalize a tensor image given the original mean and standard deviation.\n",
    "    \"\"\"\n",
    "    img_tensor = img_tensor.clone().detach().cpu()\n",
    "    for t, m, s in zip(img_tensor, mean, std):\n",
    "        t.mul_(s).add_(m)\n",
    "    return img_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Visualize Y_Pred, Y pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_output(model, data, device, idx, normalize=True):\n",
    "    ''' Plots the input, ground truth, and model output of a given ID in the dataset. \n",
    "        ID must be within (0, len(dataset)). \n",
    "        Unnormalizes the image if\n",
    "    '''\n",
    "    if idx >= len(data): raise IndexError(\"idx out of bounds of dataset length\")\n",
    "\n",
    "    noisy_image, clean_image = data[idx] \n",
    "    noisy_image = noisy_image.unsqueeze(0).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        predicted_image = model(noisy_image).cpu().squeeze(0) #create predicted image from model \n",
    "    if normalize:\n",
    "        noisy_image = unnormalize(noisy_image.cpu().squeeze(0), normalization_mean, normalization_std)\n",
    "        clean_image = unnormalize(clean_image, normalization_mean, normalization_std)\n",
    "        #unnormalize predicted image? yes/no?\n",
    "    else:\n",
    "        noisy_image = noisy_image.cpu().squeeze(0) #if no normalization for noisy_image\n",
    "    \n",
    "    predicted_image = predicted_image.permute(1, 2, 0).clip(0, 1)  # Transpose for plotting\n",
    "\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(noisy_image.permute(1, 2, 0))\n",
    "    plt.title(\"Noisy Input\")\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(predicted_image)\n",
    "    plt.title(\"Model Output\")\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.imshow(clean_image.permute(1, 2, 0))\n",
    "    plt.title(\"Ground Truth\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"saved_models/best_denoising_model.pth\"\n",
    "model_vis = DenoisingCNN()\n",
    "model_vis.load_state_dict(torch.load(model_path))\n",
    "model_vis.eval()\n",
    "model_vis.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_output(model=model_vis, data=train_data, device=device, idx=26, normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
