{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/micromamba/envs/deepl/lib/python3.10/site-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 1.4.21 (you have 1.4.15). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from ultralytics import YOLO\n",
    "from datasets import YOLODataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function train in module ultralytics.engine.model:\n",
      "\n",
      "train(self, trainer=None, **kwargs)\n",
      "    Trains the model using the specified dataset and training configuration.\n",
      "    \n",
      "    This method facilitates model training with a range of customizable settings. It supports training with a\n",
      "    custom trainer or the default training approach. The method handles scenarios such as resuming training\n",
      "    from a checkpoint, integrating with Ultralytics HUB, and updating model and configuration after training.\n",
      "    \n",
      "    When using Ultralytics HUB, if the session has a loaded model, the method prioritizes HUB training\n",
      "    arguments and warns if local arguments are provided. It checks for pip updates and combines default\n",
      "    configurations, method-specific defaults, and user-provided arguments to configure the training process.\n",
      "    \n",
      "    Args:\n",
      "        trainer (BaseTrainer | None): Custom trainer instance for model training. If None, uses default.\n",
      "        **kwargs (Any): Arbitrary keyword arguments for training configuration. Common options include:\n",
      "            data (str): Path to dataset configuration file.\n",
      "            epochs (int): Number of training epochs.\n",
      "            batch_size (int): Batch size for training.\n",
      "            imgsz (int): Input image size.\n",
      "            device (str): Device to run training on (e.g., 'cuda', 'cpu').\n",
      "            workers (int): Number of worker threads for data loading.\n",
      "            optimizer (str): Optimizer to use for training.\n",
      "            lr0 (float): Initial learning rate.\n",
      "            patience (int): Epochs to wait for no observable improvement for early stopping of training.\n",
      "    \n",
      "    Returns:\n",
      "        (Dict | None): Training metrics if available and training is successful; otherwise, None.\n",
      "    \n",
      "    Raises:\n",
      "        AssertionError: If the model is not a PyTorch model.\n",
      "        PermissionError: If there is a permission issue with the HUB session.\n",
      "        ModuleNotFoundError: If the HUB SDK is not installed.\n",
      "    \n",
      "    Examples:\n",
      "        >>> model = YOLO(\"yolo11n.pt\")\n",
      "        >>> results = model.train(data=\"coco8.yaml\", epochs=3)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(YOLO.train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 448\n",
      "Validation samples: 127\n",
      "Test samples: 63\n"
     ]
    }
   ],
   "source": [
    "# Root directory of the dataset\n",
    "class_names = ['fish', 'jellyfish', 'penguin', 'puffin', 'shark', 'starfish', 'stingray']\n",
    "num_classes = len(class_names)\n",
    "root_dir = 'datasets/aquarium-data-cots/aquarium_pretrain'\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = YOLODataset(root_dir, split='train', num_classes=num_classes)\n",
    "valid_dataset = YOLODataset(root_dir, split='valid', num_classes=num_classes)\n",
    "test_dataset = YOLODataset(root_dir, split='test', num_classes=num_classes)\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(valid_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = YOLO('yolov5su.pt')\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.3.28 available ðŸ˜ƒ Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.27 ðŸš€ Python-3.10.15 torch-2.5.1+cu124 CUDA:0 (NVIDIA L4, 22478MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov5su.pt, data=datasets/aquarium-data-cots/aquarium_pretrain/data.yaml, epochs=10, time=None, patience=100, batch=16, imgsz=640, save=True, save_period=1, cache=False, device=cuda:0, workers=8, project=None, name=train133, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.0, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train133\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1      3520  ultralytics.nn.modules.conv.Conv             [3, 32, 6, 2, 2]              \n",
      "  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  2                  -1  1     18816  ultralytics.nn.modules.block.C3              [64, 64, 1]                   \n",
      "  3                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  4                  -1  2    115712  ultralytics.nn.modules.block.C3              [128, 128, 2]                 \n",
      "  5                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  6                  -1  3    625152  ultralytics.nn.modules.block.C3              [256, 256, 3]                 \n",
      "  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
      "  8                  -1  1   1182720  ultralytics.nn.modules.block.C3              [512, 512, 1]                 \n",
      "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
      " 10                  -1  1    131584  ultralytics.nn.modules.conv.Conv             [512, 256, 1, 1]              \n",
      " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 13                  -1  1    361984  ultralytics.nn.modules.block.C3              [512, 256, 1, False]          \n",
      " 14                  -1  1     33024  ultralytics.nn.modules.conv.Conv             [256, 128, 1, 1]              \n",
      " 15                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 16             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 17                  -1  1     90880  ultralytics.nn.modules.block.C3              [256, 128, 1, False]          \n",
      " 18                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 19            [-1, 14]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 20                  -1  1    296448  ultralytics.nn.modules.block.C3              [256, 256, 1, False]          \n",
      " 21                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      " 22            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 23                  -1  1   1182720  ultralytics.nn.modules.block.C3              [512, 512, 1, False]          \n",
      " 24        [17, 20, 23]  1   2118757  ultralytics.nn.modules.head.Detect           [7, [128, 256, 512]]          \n",
      "YOLOv5s summary: 262 layers, 9,124,901 parameters, 9,124,885 gradients, 24.1 GFLOPs\n",
      "\n",
      "Transferred 82/427 items from pretrained weights\n",
      "Freezing layer 'model.24.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/ubuntu/cs230_proj/datasets/aquarium-data-cots/aquarium_pretrain/train/labels.cache... 448 images, 1 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 448/448 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, num_output_channels=3, method='weighted_average'), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/ubuntu/cs230_proj/datasets/aquarium-data-cots/aquarium_pretrain/valid/labels.cache... 127 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 127/127 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs/detect/train133/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000909, momentum=0.9) with parameter groups 69 weight(decay=0.0), 76 weight(decay=0.0005), 75 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 4 dataloader workers\n",
      "Logging results to \u001b[1mruns/detect/train133\u001b[0m\n",
      "Starting training for 10 epochs...\n",
      "Closing dataloader mosaic\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, num_output_channels=3, method='weighted_average'), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/10      3.89G      2.298      3.299       1.89        129        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:04<00:00,  5.85it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00,  5.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        127        909      0.471        0.2      0.179     0.0975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       2/10      3.92G      1.727      2.113      1.404        114        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:04<00:00,  6.78it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00,  6.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        127        909      0.508       0.32      0.297      0.161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       3/10      3.92G      1.546      1.639      1.286        110        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:04<00:00,  6.85it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00,  6.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        127        909      0.582      0.465      0.454       0.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       4/10      3.86G      1.474      1.509      1.232         90        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:04<00:00,  6.95it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00,  6.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        127        909      0.709      0.536      0.608      0.345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       5/10      3.89G       1.41      1.397      1.191        100        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:04<00:00,  6.92it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00,  6.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        127        909      0.709      0.531      0.638       0.36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       6/10      3.87G      1.423       1.33      1.193        138        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:04<00:00,  6.88it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00,  6.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        127        909      0.708        0.6      0.644      0.361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       7/10      3.89G      1.375      1.276      1.186        167        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:04<00:00,  6.87it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00,  6.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        127        909      0.713      0.566      0.671      0.381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       8/10      3.79G      1.344      1.242       1.17         87        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:04<00:00,  6.82it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00,  6.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        127        909      0.711       0.61      0.686      0.398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       9/10       3.9G      1.338      1.215      1.156        134        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:04<00:00,  6.90it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00,  6.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        127        909       0.78      0.599      0.686      0.396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      10/10      3.88G      1.356      1.201      1.157        136        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:04<00:00,  6.87it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00,  6.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        127        909      0.774      0.608      0.697      0.408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "10 epochs completed in 0.016 hours.\n",
      "Optimizer stripped from runs/detect/train133/weights/last.pt, 18.5MB\n",
      "Optimizer stripped from runs/detect/train133/weights/best.pt, 18.5MB\n",
      "\n",
      "Validating runs/detect/train133/weights/best.pt...\n",
      "Ultralytics 8.3.27 ðŸš€ Python-3.10.15 torch-2.5.1+cu124 CUDA:0 (NVIDIA L4, 22478MiB)\n",
      "YOLOv5s summary (fused): 193 layers, 9,114,245 parameters, 0 gradients, 23.8 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:01<00:00,  3.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        127        909      0.774      0.608      0.697      0.408\n",
      "                  fish         63        459      0.844      0.613      0.746      0.406\n",
      "             jellyfish          9        155      0.785       0.85      0.882      0.491\n",
      "               penguin         17        104      0.675      0.663      0.665      0.291\n",
      "                puffin         15         74      0.769      0.404      0.558      0.272\n",
      "                 shark         28         57      0.679      0.561      0.659      0.387\n",
      "              starfish         17         27      0.947      0.556      0.659      0.507\n",
      "              stingray         23         33      0.722      0.606      0.707      0.503\n",
      "Speed: 0.2ms preprocess, 2.0ms inference, 0.0ms loss, 1.5ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/train133\u001b[0m\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'YOLO' object has no attribute 'plot'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 19\u001b[0m\n\u001b[1;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain(\n\u001b[1;32m      4\u001b[0m     data\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdatasets/aquarium-data-cots/aquarium_pretrain/data.yaml\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m      5\u001b[0m     epochs\u001b[38;5;241m=\u001b[39mnum_epochs, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m     save_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mruns/train\u001b[39m\u001b[38;5;124m'\u001b[39m,  \u001b[38;5;66;03m# Directory to save training results\u001b[39;00m\n\u001b[1;32m      9\u001b[0m )\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Evaluate metrics on validation set after each epoch\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# for epoch in range(num_epochs):\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m#     val_metrics = model.val()\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     17\u001b[0m \n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Plot metrics like loss and mAP\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m()\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Save predictions to a directory\u001b[39;00m\n\u001b[1;32m     22\u001b[0m model\u001b[38;5;241m.\u001b[39mpredict(valid_loader, conf\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.8\u001b[39m, save\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, save_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mruns/predict/valid\u001b[39m\u001b[38;5;124m'\u001b[39m)  \n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1931\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1929\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1930\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1931\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m   1932\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1933\u001b[0m )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'YOLO' object has no attribute 'plot'"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "model.train(\n",
    "    data='datasets/aquarium-data-cots/aquarium_pretrain/data.yaml', \n",
    "    epochs=num_epochs, \n",
    "    imgsz=640,  \n",
    "    save_period=1,  # Save model every epoch\n",
    "    save_dir='runs/train',  # Directory to save training results\n",
    ")\n",
    "\n",
    "# Evaluate metrics on validation set after each epoch\n",
    "# for epoch in range(num_epochs):\n",
    "#     val_metrics = model.val()\n",
    "#     print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "#     print(\"Validation Loss:\", val_metrics['loss'])\n",
    "#     print(\"Validation Accuracy:\", val_metrics['metrics/accuracy'])\n",
    "\n",
    "# Plot metrics like loss and mAP\n",
    "# model.plot()\n",
    "\n",
    "# Save predictions to a directory\n",
    "# model.predict(valid_loader, conf=0.8, save=True, save_dir='runs/predict/valid')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class YOLO in module ultralytics.models.yolo.model:\n",
      "\n",
      "class YOLO(ultralytics.engine.model.Model)\n",
      " |  YOLO(model='yolo11n.pt', task=None, verbose=False)\n",
      " |  \n",
      " |  YOLO (You Only Look Once) object detection model.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      YOLO\n",
      " |      ultralytics.engine.model.Model\n",
      " |      torch.nn.modules.module.Module\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, model='yolo11n.pt', task=None, verbose=False)\n",
      " |      Initialize YOLO model, switching to YOLOWorld if model filename contains '-world'.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties defined here:\n",
      " |  \n",
      " |  task_map\n",
      " |      Map head to model, trainer, validator, and predictor classes.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __annotations__ = {}\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from ultralytics.engine.model.Model:\n",
      " |  \n",
      " |  __call__(self, source: Union[str, pathlib.Path, int, PIL.Image.Image, list, tuple, numpy.ndarray, torch.Tensor] = None, stream: bool = False, **kwargs) -> list\n",
      " |      Alias for the predict method, enabling the model instance to be callable for predictions.\n",
      " |      \n",
      " |      This method simplifies the process of making predictions by allowing the model instance to be called\n",
      " |      directly with the required arguments.\n",
      " |      \n",
      " |      Args:\n",
      " |          source (str | Path | int | PIL.Image | np.ndarray | torch.Tensor | List | Tuple): The source of\n",
      " |              the image(s) to make predictions on. Can be a file path, URL, PIL image, numpy array, PyTorch\n",
      " |              tensor, or a list/tuple of these.\n",
      " |          stream (bool): If True, treat the input source as a continuous stream for predictions.\n",
      " |          **kwargs (Any): Additional keyword arguments to configure the prediction process.\n",
      " |      \n",
      " |      Returns:\n",
      " |          (List[ultralytics.engine.results.Results]): A list of prediction results, each encapsulated in a\n",
      " |              Results object.\n",
      " |      \n",
      " |      Examples:\n",
      " |          >>> model = YOLO(\"yolo11n.pt\")\n",
      " |          >>> results = model(\"https://ultralytics.com/images/bus.jpg\")\n",
      " |          >>> for r in results:\n",
      " |          ...     print(f\"Detected {len(r)} objects in image\")\n",
      " |  \n",
      " |  add_callback(self, event: str, func) -> None\n",
      " |      Adds a callback function for a specified event.\n",
      " |      \n",
      " |      This method allows registering custom callback functions that are triggered on specific events during\n",
      " |      model operations such as training or inference. Callbacks provide a way to extend and customize the\n",
      " |      behavior of the model at various stages of its lifecycle.\n",
      " |      \n",
      " |      Args:\n",
      " |          event (str): The name of the event to attach the callback to. Must be a valid event name recognized\n",
      " |              by the Ultralytics framework.\n",
      " |          func (Callable): The callback function to be registered. This function will be called when the\n",
      " |              specified event occurs.\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValueError: If the event name is not recognized or is invalid.\n",
      " |      \n",
      " |      Examples:\n",
      " |          >>> def on_train_start(trainer):\n",
      " |          ...     print(\"Training is starting!\")\n",
      " |          >>> model = YOLO(\"yolo11n.pt\")\n",
      " |          >>> model.add_callback(\"on_train_start\", on_train_start)\n",
      " |          >>> model.train(data=\"coco8.yaml\", epochs=1)\n",
      " |  \n",
      " |  benchmark(self, **kwargs)\n",
      " |      Benchmarks the model across various export formats to evaluate performance.\n",
      " |      \n",
      " |      This method assesses the model's performance in different export formats, such as ONNX, TorchScript, etc.\n",
      " |      It uses the 'benchmark' function from the ultralytics.utils.benchmarks module. The benchmarking is\n",
      " |      configured using a combination of default configuration values, model-specific arguments, method-specific\n",
      " |      defaults, and any additional user-provided keyword arguments.\n",
      " |      \n",
      " |      Args:\n",
      " |          **kwargs (Any): Arbitrary keyword arguments to customize the benchmarking process. These are combined with\n",
      " |              default configurations, model-specific arguments, and method defaults. Common options include:\n",
      " |              - data (str): Path to the dataset for benchmarking.\n",
      " |              - imgsz (int | List[int]): Image size for benchmarking.\n",
      " |              - half (bool): Whether to use half-precision (FP16) mode.\n",
      " |              - int8 (bool): Whether to use int8 precision mode.\n",
      " |              - device (str): Device to run the benchmark on (e.g., 'cpu', 'cuda').\n",
      " |              - verbose (bool): Whether to print detailed benchmark information.\n",
      " |      \n",
      " |      Returns:\n",
      " |          (Dict): A dictionary containing the results of the benchmarking process, including metrics for\n",
      " |              different export formats.\n",
      " |      \n",
      " |      Raises:\n",
      " |          AssertionError: If the model is not a PyTorch model.\n",
      " |      \n",
      " |      Examples:\n",
      " |          >>> model = YOLO(\"yolo11n.pt\")\n",
      " |          >>> results = model.benchmark(data=\"coco8.yaml\", imgsz=640, half=True)\n",
      " |          >>> print(results)\n",
      " |  \n",
      " |  clear_callback(self, event: str) -> None\n",
      " |      Clears all callback functions registered for a specified event.\n",
      " |      \n",
      " |      This method removes all custom and default callback functions associated with the given event.\n",
      " |      It resets the callback list for the specified event to an empty list, effectively removing all\n",
      " |      registered callbacks for that event.\n",
      " |      \n",
      " |      Args:\n",
      " |          event (str): The name of the event for which to clear the callbacks. This should be a valid event name\n",
      " |              recognized by the Ultralytics callback system.\n",
      " |      \n",
      " |      Examples:\n",
      " |          >>> model = YOLO(\"yolo11n.pt\")\n",
      " |          >>> model.add_callback(\"on_train_start\", lambda: print(\"Training started\"))\n",
      " |          >>> model.clear_callback(\"on_train_start\")\n",
      " |          >>> # All callbacks for 'on_train_start' are now removed\n",
      " |      \n",
      " |      Notes:\n",
      " |          - This method affects both custom callbacks added by the user and default callbacks\n",
      " |            provided by the Ultralytics framework.\n",
      " |          - After calling this method, no callbacks will be executed for the specified event\n",
      " |            until new ones are added.\n",
      " |          - Use with caution as it removes all callbacks, including essential ones that might\n",
      " |            be required for proper functioning of certain operations.\n",
      " |  \n",
      " |  embed(self, source: Union[str, pathlib.Path, int, list, tuple, numpy.ndarray, torch.Tensor] = None, stream: bool = False, **kwargs) -> list\n",
      " |      Generates image embeddings based on the provided source.\n",
      " |      \n",
      " |      This method is a wrapper around the 'predict()' method, focusing on generating embeddings from an image\n",
      " |      source. It allows customization of the embedding process through various keyword arguments.\n",
      " |      \n",
      " |      Args:\n",
      " |          source (str | Path | int | List | Tuple | np.ndarray | torch.Tensor): The source of the image for\n",
      " |              generating embeddings. Can be a file path, URL, PIL image, numpy array, etc.\n",
      " |          stream (bool): If True, predictions are streamed.\n",
      " |          **kwargs (Any): Additional keyword arguments for configuring the embedding process.\n",
      " |      \n",
      " |      Returns:\n",
      " |          (List[torch.Tensor]): A list containing the image embeddings.\n",
      " |      \n",
      " |      Raises:\n",
      " |          AssertionError: If the model is not a PyTorch model.\n",
      " |      \n",
      " |      Examples:\n",
      " |          >>> model = YOLO(\"yolo11n.pt\")\n",
      " |          >>> image = \"https://ultralytics.com/images/bus.jpg\"\n",
      " |          >>> embeddings = model.embed(image)\n",
      " |          >>> print(embeddings[0].shape)\n",
      " |  \n",
      " |  export(self, **kwargs) -> str\n",
      " |      Exports the model to a different format suitable for deployment.\n",
      " |      \n",
      " |      This method facilitates the export of the model to various formats (e.g., ONNX, TorchScript) for deployment\n",
      " |      purposes. It uses the 'Exporter' class for the export process, combining model-specific overrides, method\n",
      " |      defaults, and any additional arguments provided.\n",
      " |      \n",
      " |      Args:\n",
      " |          **kwargs (Dict): Arbitrary keyword arguments to customize the export process. These are combined with\n",
      " |              the model's overrides and method defaults. Common arguments include:\n",
      " |              format (str): Export format (e.g., 'onnx', 'engine', 'coreml').\n",
      " |              half (bool): Export model in half-precision.\n",
      " |              int8 (bool): Export model in int8 precision.\n",
      " |              device (str): Device to run the export on.\n",
      " |              workspace (int): Maximum memory workspace size for TensorRT engines.\n",
      " |              nms (bool): Add Non-Maximum Suppression (NMS) module to model.\n",
      " |              simplify (bool): Simplify ONNX model.\n",
      " |      \n",
      " |      Returns:\n",
      " |          (str): The path to the exported model file.\n",
      " |      \n",
      " |      Raises:\n",
      " |          AssertionError: If the model is not a PyTorch model.\n",
      " |          ValueError: If an unsupported export format is specified.\n",
      " |          RuntimeError: If the export process fails due to errors.\n",
      " |      \n",
      " |      Examples:\n",
      " |          >>> model = YOLO(\"yolo11n.pt\")\n",
      " |          >>> model.export(format=\"onnx\", dynamic=True, simplify=True)\n",
      " |          'path/to/exported/model.onnx'\n",
      " |  \n",
      " |  fuse(self)\n",
      " |      Fuses Conv2d and BatchNorm2d layers in the model for optimized inference.\n",
      " |      \n",
      " |      This method iterates through the model's modules and fuses consecutive Conv2d and BatchNorm2d layers\n",
      " |      into a single layer. This fusion can significantly improve inference speed by reducing the number of\n",
      " |      operations and memory accesses required during forward passes.\n",
      " |      \n",
      " |      The fusion process typically involves folding the BatchNorm2d parameters (mean, variance, weight, and\n",
      " |      bias) into the preceding Conv2d layer's weights and biases. This results in a single Conv2d layer that\n",
      " |      performs both convolution and normalization in one step.\n",
      " |      \n",
      " |      Raises:\n",
      " |          TypeError: If the model is not a PyTorch nn.Module.\n",
      " |      \n",
      " |      Examples:\n",
      " |          >>> model = Model(\"yolo11n.pt\")\n",
      " |          >>> model.fuse()\n",
      " |          >>> # Model is now fused and ready for optimized inference\n",
      " |  \n",
      " |  info(self, detailed: bool = False, verbose: bool = True)\n",
      " |      Logs or returns model information.\n",
      " |      \n",
      " |      This method provides an overview or detailed information about the model, depending on the arguments\n",
      " |      passed. It can control the verbosity of the output and return the information as a list.\n",
      " |      \n",
      " |      Args:\n",
      " |          detailed (bool): If True, shows detailed information about the model layers and parameters.\n",
      " |          verbose (bool): If True, prints the information. If False, returns the information as a list.\n",
      " |      \n",
      " |      Returns:\n",
      " |          (List[str]): A list of strings containing various types of information about the model, including\n",
      " |              model summary, layer details, and parameter counts. Empty if verbose is True.\n",
      " |      \n",
      " |      Raises:\n",
      " |          TypeError: If the model is not a PyTorch model.\n",
      " |      \n",
      " |      Examples:\n",
      " |          >>> model = Model(\"yolo11n.pt\")\n",
      " |          >>> model.info()  # Prints model summary\n",
      " |          >>> info_list = model.info(detailed=True, verbose=False)  # Returns detailed info as a list\n",
      " |  \n",
      " |  load(self, weights: Union[str, pathlib.Path] = 'yolo11n.pt') -> 'Model'\n",
      " |      Loads parameters from the specified weights file into the model.\n",
      " |      \n",
      " |      This method supports loading weights from a file or directly from a weights object. It matches parameters by\n",
      " |      name and shape and transfers them to the model.\n",
      " |      \n",
      " |      Args:\n",
      " |          weights (Union[str, Path]): Path to the weights file or a weights object.\n",
      " |      \n",
      " |      Returns:\n",
      " |          (Model): The instance of the class with loaded weights.\n",
      " |      \n",
      " |      Raises:\n",
      " |          AssertionError: If the model is not a PyTorch model.\n",
      " |      \n",
      " |      Examples:\n",
      " |          >>> model = Model()\n",
      " |          >>> model.load(\"yolo11n.pt\")\n",
      " |          >>> model.load(Path(\"path/to/weights.pt\"))\n",
      " |  \n",
      " |  predict(self, source: Union[str, pathlib.Path, int, PIL.Image.Image, list, tuple, numpy.ndarray, torch.Tensor] = None, stream: bool = False, predictor=None, **kwargs) -> List[ultralytics.engine.results.Results]\n",
      " |      Performs predictions on the given image source using the YOLO model.\n",
      " |      \n",
      " |      This method facilitates the prediction process, allowing various configurations through keyword arguments.\n",
      " |      It supports predictions with custom predictors or the default predictor method. The method handles different\n",
      " |      types of image sources and can operate in a streaming mode.\n",
      " |      \n",
      " |      Args:\n",
      " |          source (str | Path | int | PIL.Image | np.ndarray | torch.Tensor | List | Tuple): The source\n",
      " |              of the image(s) to make predictions on. Accepts various types including file paths, URLs, PIL\n",
      " |              images, numpy arrays, and torch tensors.\n",
      " |          stream (bool): If True, treats the input source as a continuous stream for predictions.\n",
      " |          predictor (BasePredictor | None): An instance of a custom predictor class for making predictions.\n",
      " |              If None, the method uses a default predictor.\n",
      " |          **kwargs (Any): Additional keyword arguments for configuring the prediction process.\n",
      " |      \n",
      " |      Returns:\n",
      " |          (List[ultralytics.engine.results.Results]): A list of prediction results, each encapsulated in a\n",
      " |              Results object.\n",
      " |      \n",
      " |      Examples:\n",
      " |          >>> model = YOLO(\"yolo11n.pt\")\n",
      " |          >>> results = model.predict(source=\"path/to/image.jpg\", conf=0.25)\n",
      " |          >>> for r in results:\n",
      " |          ...     print(r.boxes.data)  # print detection bounding boxes\n",
      " |      \n",
      " |      Notes:\n",
      " |          - If 'source' is not provided, it defaults to the ASSETS constant with a warning.\n",
      " |          - The method sets up a new predictor if not already present and updates its arguments with each call.\n",
      " |          - For SAM-type models, 'prompts' can be passed as a keyword argument.\n",
      " |  \n",
      " |  reset_callbacks(self) -> None\n",
      " |      Resets all callbacks to their default functions.\n",
      " |      \n",
      " |      This method reinstates the default callback functions for all events, removing any custom callbacks that were\n",
      " |      previously added. It iterates through all default callback events and replaces the current callbacks with the\n",
      " |      default ones.\n",
      " |      \n",
      " |      The default callbacks are defined in the 'callbacks.default_callbacks' dictionary, which contains predefined\n",
      " |      functions for various events in the model's lifecycle, such as on_train_start, on_epoch_end, etc.\n",
      " |      \n",
      " |      This method is useful when you want to revert to the original set of callbacks after making custom\n",
      " |      modifications, ensuring consistent behavior across different runs or experiments.\n",
      " |      \n",
      " |      Examples:\n",
      " |          >>> model = YOLO(\"yolo11n.pt\")\n",
      " |          >>> model.add_callback(\"on_train_start\", custom_function)\n",
      " |          >>> model.reset_callbacks()\n",
      " |          # All callbacks are now reset to their default functions\n",
      " |  \n",
      " |  reset_weights(self) -> 'Model'\n",
      " |      Resets the model's weights to their initial state.\n",
      " |      \n",
      " |      This method iterates through all modules in the model and resets their parameters if they have a\n",
      " |      'reset_parameters' method. It also ensures that all parameters have 'requires_grad' set to True,\n",
      " |      enabling them to be updated during training.\n",
      " |      \n",
      " |      Returns:\n",
      " |          (Model): The instance of the class with reset weights.\n",
      " |      \n",
      " |      Raises:\n",
      " |          AssertionError: If the model is not a PyTorch model.\n",
      " |      \n",
      " |      Examples:\n",
      " |          >>> model = Model(\"yolo11n.pt\")\n",
      " |          >>> model.reset_weights()\n",
      " |  \n",
      " |  save(self, filename: Union[str, pathlib.Path] = 'saved_model.pt') -> None\n",
      " |      Saves the current model state to a file.\n",
      " |      \n",
      " |      This method exports the model's checkpoint (ckpt) to the specified filename. It includes metadata such as\n",
      " |      the date, Ultralytics version, license information, and a link to the documentation.\n",
      " |      \n",
      " |      Args:\n",
      " |          filename (Union[str, Path]): The name of the file to save the model to.\n",
      " |      \n",
      " |      Raises:\n",
      " |          AssertionError: If the model is not a PyTorch model.\n",
      " |      \n",
      " |      Examples:\n",
      " |          >>> model = Model(\"yolo11n.pt\")\n",
      " |          >>> model.save(\"my_model.pt\")\n",
      " |  \n",
      " |  track(self, source: Union[str, pathlib.Path, int, list, tuple, numpy.ndarray, torch.Tensor] = None, stream: bool = False, persist: bool = False, **kwargs) -> List[ultralytics.engine.results.Results]\n",
      " |      Conducts object tracking on the specified input source using the registered trackers.\n",
      " |      \n",
      " |      This method performs object tracking using the model's predictors and optionally registered trackers. It handles\n",
      " |      various input sources such as file paths or video streams, and supports customization through keyword arguments.\n",
      " |      The method registers trackers if not already present and can persist them between calls.\n",
      " |      \n",
      " |      Args:\n",
      " |          source (Union[str, Path, int, List, Tuple, np.ndarray, torch.Tensor], optional): Input source for object\n",
      " |              tracking. Can be a file path, URL, or video stream.\n",
      " |          stream (bool): If True, treats the input source as a continuous video stream. Defaults to False.\n",
      " |          persist (bool): If True, persists trackers between different calls to this method. Defaults to False.\n",
      " |          **kwargs (Any): Additional keyword arguments for configuring the tracking process.\n",
      " |      \n",
      " |      Returns:\n",
      " |          (List[ultralytics.engine.results.Results]): A list of tracking results, each a Results object.\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: If the predictor does not have registered trackers.\n",
      " |      \n",
      " |      Examples:\n",
      " |          >>> model = YOLO(\"yolo11n.pt\")\n",
      " |          >>> results = model.track(source=\"path/to/video.mp4\", show=True)\n",
      " |          >>> for r in results:\n",
      " |          ...     print(r.boxes.id)  # print tracking IDs\n",
      " |      \n",
      " |      Notes:\n",
      " |          - This method sets a default confidence threshold of 0.1 for ByteTrack-based tracking.\n",
      " |          - The tracking mode is explicitly set in the keyword arguments.\n",
      " |          - Batch size is set to 1 for tracking in videos.\n",
      " |  \n",
      " |  train(self, trainer=None, **kwargs)\n",
      " |      Trains the model using the specified dataset and training configuration.\n",
      " |      \n",
      " |      This method facilitates model training with a range of customizable settings. It supports training with a\n",
      " |      custom trainer or the default training approach. The method handles scenarios such as resuming training\n",
      " |      from a checkpoint, integrating with Ultralytics HUB, and updating model and configuration after training.\n",
      " |      \n",
      " |      When using Ultralytics HUB, if the session has a loaded model, the method prioritizes HUB training\n",
      " |      arguments and warns if local arguments are provided. It checks for pip updates and combines default\n",
      " |      configurations, method-specific defaults, and user-provided arguments to configure the training process.\n",
      " |      \n",
      " |      Args:\n",
      " |          trainer (BaseTrainer | None): Custom trainer instance for model training. If None, uses default.\n",
      " |          **kwargs (Any): Arbitrary keyword arguments for training configuration. Common options include:\n",
      " |              data (str): Path to dataset configuration file.\n",
      " |              epochs (int): Number of training epochs.\n",
      " |              batch_size (int): Batch size for training.\n",
      " |              imgsz (int): Input image size.\n",
      " |              device (str): Device to run training on (e.g., 'cuda', 'cpu').\n",
      " |              workers (int): Number of worker threads for data loading.\n",
      " |              optimizer (str): Optimizer to use for training.\n",
      " |              lr0 (float): Initial learning rate.\n",
      " |              patience (int): Epochs to wait for no observable improvement for early stopping of training.\n",
      " |      \n",
      " |      Returns:\n",
      " |          (Dict | None): Training metrics if available and training is successful; otherwise, None.\n",
      " |      \n",
      " |      Raises:\n",
      " |          AssertionError: If the model is not a PyTorch model.\n",
      " |          PermissionError: If there is a permission issue with the HUB session.\n",
      " |          ModuleNotFoundError: If the HUB SDK is not installed.\n",
      " |      \n",
      " |      Examples:\n",
      " |          >>> model = YOLO(\"yolo11n.pt\")\n",
      " |          >>> results = model.train(data=\"coco8.yaml\", epochs=3)\n",
      " |  \n",
      " |  tune(self, use_ray=False, iterations=10, *args, **kwargs)\n",
      " |      Conducts hyperparameter tuning for the model, with an option to use Ray Tune.\n",
      " |      \n",
      " |      This method supports two modes of hyperparameter tuning: using Ray Tune or a custom tuning method.\n",
      " |      When Ray Tune is enabled, it leverages the 'run_ray_tune' function from the ultralytics.utils.tuner module.\n",
      " |      Otherwise, it uses the internal 'Tuner' class for tuning. The method combines default, overridden, and\n",
      " |      custom arguments to configure the tuning process.\n",
      " |      \n",
      " |      Args:\n",
      " |          use_ray (bool): If True, uses Ray Tune for hyperparameter tuning. Defaults to False.\n",
      " |          iterations (int): The number of tuning iterations to perform. Defaults to 10.\n",
      " |          *args (List): Variable length argument list for additional arguments.\n",
      " |          **kwargs (Dict): Arbitrary keyword arguments. These are combined with the model's overrides and defaults.\n",
      " |      \n",
      " |      Returns:\n",
      " |          (Dict): A dictionary containing the results of the hyperparameter search.\n",
      " |      \n",
      " |      Raises:\n",
      " |          AssertionError: If the model is not a PyTorch model.\n",
      " |      \n",
      " |      Examples:\n",
      " |          >>> model = YOLO(\"yolo11n.pt\")\n",
      " |          >>> results = model.tune(use_ray=True, iterations=20)\n",
      " |          >>> print(results)\n",
      " |  \n",
      " |  val(self, validator=None, **kwargs)\n",
      " |      Validates the model using a specified dataset and validation configuration.\n",
      " |      \n",
      " |      This method facilitates the model validation process, allowing for customization through various settings. It\n",
      " |      supports validation with a custom validator or the default validation approach. The method combines default\n",
      " |      configurations, method-specific defaults, and user-provided arguments to configure the validation process.\n",
      " |      \n",
      " |      Args:\n",
      " |          validator (ultralytics.engine.validator.BaseValidator | None): An instance of a custom validator class for\n",
      " |              validating the model.\n",
      " |          **kwargs (Any): Arbitrary keyword arguments for customizing the validation process.\n",
      " |      \n",
      " |      Returns:\n",
      " |          (ultralytics.utils.metrics.DetMetrics): Validation metrics obtained from the validation process.\n",
      " |      \n",
      " |      Raises:\n",
      " |          AssertionError: If the model is not a PyTorch model.\n",
      " |      \n",
      " |      Examples:\n",
      " |          >>> model = YOLO(\"yolo11n.pt\")\n",
      " |          >>> results = model.val(data=\"coco8.yaml\", imgsz=640)\n",
      " |          >>> print(results.box.map)  # Print mAP50-95\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods inherited from ultralytics.engine.model.Model:\n",
      " |  \n",
      " |  is_hub_model(model: str) -> bool\n",
      " |      Check if the provided model is an Ultralytics HUB model.\n",
      " |      \n",
      " |      This static method determines whether the given model string represents a valid Ultralytics HUB model\n",
      " |      identifier.\n",
      " |      \n",
      " |      Args:\n",
      " |          model (str): The model string to check.\n",
      " |      \n",
      " |      Returns:\n",
      " |          (bool): True if the model is a valid Ultralytics HUB model, False otherwise.\n",
      " |      \n",
      " |      Examples:\n",
      " |          >>> Model.is_hub_model(\"https://hub.ultralytics.com/models/MODEL\")\n",
      " |          True\n",
      " |          >>> Model.is_hub_model(\"yolo11n.pt\")\n",
      " |          False\n",
      " |  \n",
      " |  is_triton_model(model: str) -> bool\n",
      " |      Checks if the given model string is a Triton Server URL.\n",
      " |      \n",
      " |      This static method determines whether the provided model string represents a valid Triton Server URL by\n",
      " |      parsing its components using urllib.parse.urlsplit().\n",
      " |      \n",
      " |      Args:\n",
      " |          model (str): The model string to be checked.\n",
      " |      \n",
      " |      Returns:\n",
      " |          (bool): True if the model string is a valid Triton Server URL, False otherwise.\n",
      " |      \n",
      " |      Examples:\n",
      " |          >>> Model.is_triton_model(\"http://localhost:8000/v2/models/yolov8n\")\n",
      " |          True\n",
      " |          >>> Model.is_triton_model(\"yolo11n.pt\")\n",
      " |          False\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from ultralytics.engine.model.Model:\n",
      " |  \n",
      " |  device\n",
      " |      Retrieves the device on which the model's parameters are allocated.\n",
      " |      \n",
      " |      This property determines the device (CPU or GPU) where the model's parameters are currently stored. It is\n",
      " |      applicable only to models that are instances of nn.Module.\n",
      " |      \n",
      " |      Returns:\n",
      " |          (torch.device): The device (CPU/GPU) of the model.\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: If the model is not a PyTorch nn.Module instance.\n",
      " |      \n",
      " |      Examples:\n",
      " |          >>> model = YOLO(\"yolo11n.pt\")\n",
      " |          >>> print(model.device)\n",
      " |          device(type='cuda', index=0)  # if CUDA is available\n",
      " |          >>> model = model.to(\"cpu\")\n",
      " |          >>> print(model.device)\n",
      " |          device(type='cpu')\n",
      " |  \n",
      " |  names\n",
      " |      Retrieves the class names associated with the loaded model.\n",
      " |      \n",
      " |      This property returns the class names if they are defined in the model. It checks the class names for validity\n",
      " |      using the 'check_class_names' function from the ultralytics.nn.autobackend module. If the predictor is not\n",
      " |      initialized, it sets it up before retrieving the names.\n",
      " |      \n",
      " |      Returns:\n",
      " |          (Dict[int, str]): A dict of class names associated with the model.\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: If the model or predictor does not have a 'names' attribute.\n",
      " |      \n",
      " |      Examples:\n",
      " |          >>> model = YOLO(\"yolo11n.pt\")\n",
      " |          >>> print(model.names)\n",
      " |          {0: 'person', 1: 'bicycle', 2: 'car', ...}\n",
      " |  \n",
      " |  transforms\n",
      " |      Retrieves the transformations applied to the input data of the loaded model.\n",
      " |      \n",
      " |      This property returns the transformations if they are defined in the model. The transforms\n",
      " |      typically include preprocessing steps like resizing, normalization, and data augmentation\n",
      " |      that are applied to input data before it is fed into the model.\n",
      " |      \n",
      " |      Returns:\n",
      " |          (object | None): The transform object of the model if available, otherwise None.\n",
      " |      \n",
      " |      Examples:\n",
      " |          >>> model = YOLO(\"yolo11n.pt\")\n",
      " |          >>> transforms = model.transforms\n",
      " |          >>> if transforms:\n",
      " |          ...     print(f\"Model transforms: {transforms}\")\n",
      " |          ... else:\n",
      " |          ...     print(\"No transforms defined for this model.\")\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  __delattr__(self, name)\n",
      " |      Implement delattr(self, name).\n",
      " |  \n",
      " |  __dir__(self)\n",
      " |      Default dir() implementation.\n",
      " |  \n",
      " |  __getattr__(self, name: str) -> Any\n",
      " |      # On the return type:\n",
      " |      # We choose to return `Any` in the `__getattr__` type signature instead of a more strict `Union[Tensor, Module]`.\n",
      " |      # This is done for better interop with various type checkers for the end users.\n",
      " |      # Having a stricter return type doesn't play nicely with `register_buffer()` and forces\n",
      " |      # people to excessively use type-ignores, asserts, casts, etc.\n",
      " |      # See full discussion on the problems with returning `Union` here\n",
      " |      # https://github.com/microsoft/pyright/issues/4213\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setattr__(self, name: str, value: Union[torch.Tensor, ForwardRef('Module')]) -> None\n",
      " |      Implement setattr(self, name, value).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  add_module(self, name: str, module: Optional[ForwardRef('Module')]) -> None\n",
      " |      Add a child module to the current module.\n",
      " |      \n",
      " |      The module can be accessed as an attribute using the given name.\n",
      " |      \n",
      " |      Args:\n",
      " |          name (str): name of the child module. The child module can be\n",
      " |              accessed from this module using the given name\n",
      " |          module (Module): child module to be added to the module.\n",
      " |  \n",
      " |  apply(self: ~T, fn: Callable[[ForwardRef('Module')], NoneType]) -> ~T\n",
      " |      Apply ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self.\n",
      " |      \n",
      " |      Typical use includes initializing the parameters of a model\n",
      " |      (see also :ref:`nn-init-doc`).\n",
      " |      \n",
      " |      Args:\n",
      " |          fn (:class:`Module` -> None): function to be applied to each submodule\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> @torch.no_grad()\n",
      " |          >>> def init_weights(m):\n",
      " |          >>>     print(m)\n",
      " |          >>>     if type(m) == nn.Linear:\n",
      " |          >>>         m.weight.fill_(1.0)\n",
      " |          >>>         print(m.weight)\n",
      " |          >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n",
      " |          >>> net.apply(init_weights)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          Parameter containing:\n",
      " |          tensor([[1., 1.],\n",
      " |                  [1., 1.]], requires_grad=True)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          Parameter containing:\n",
      " |          tensor([[1., 1.],\n",
      " |                  [1., 1.]], requires_grad=True)\n",
      " |          Sequential(\n",
      " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      " |          )\n",
      " |  \n",
      " |  bfloat16(self: ~T) -> ~T\n",
      " |      Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  buffers(self, recurse: bool = True) -> Iterator[torch.Tensor]\n",
      " |      Return an iterator over module buffers.\n",
      " |      \n",
      " |      Args:\n",
      " |          recurse (bool): if True, then yields buffers of this module\n",
      " |              and all submodules. Otherwise, yields only buffers that\n",
      " |              are direct members of this module.\n",
      " |      \n",
      " |      Yields:\n",
      " |          torch.Tensor: module buffer\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      " |          >>> for buf in model.buffers():\n",
      " |          >>>     print(type(buf), buf.size())\n",
      " |          <class 'torch.Tensor'> (20L,)\n",
      " |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      " |  \n",
      " |  children(self) -> Iterator[ForwardRef('Module')]\n",
      " |      Return an iterator over immediate children modules.\n",
      " |      \n",
      " |      Yields:\n",
      " |          Module: a child module\n",
      " |  \n",
      " |  compile(self, *args, **kwargs)\n",
      " |      Compile this Module's forward using :func:`torch.compile`.\n",
      " |      \n",
      " |      This Module's `__call__` method is compiled and all arguments are passed as-is\n",
      " |      to :func:`torch.compile`.\n",
      " |      \n",
      " |      See :func:`torch.compile` for details on the arguments for this function.\n",
      " |  \n",
      " |  cpu(self: ~T) -> ~T\n",
      " |      Move all model parameters and buffers to the CPU.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  cuda(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      " |      Move all model parameters and buffers to the GPU.\n",
      " |      \n",
      " |      This also makes associated parameters and buffers different objects. So\n",
      " |      it should be called before constructing optimizer if the module will\n",
      " |      live on GPU while being optimized.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Args:\n",
      " |          device (int, optional): if specified, all parameters will be\n",
      " |              copied to that device\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  double(self: ~T) -> ~T\n",
      " |      Casts all floating point parameters and buffers to ``double`` datatype.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  eval(self: ~T) -> ~T\n",
      " |      Set the module in evaluation mode.\n",
      " |      \n",
      " |      This has any effect only on certain modules. See documentations of\n",
      " |      particular modules for details of their behaviors in training/evaluation\n",
      " |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      " |      etc.\n",
      " |      \n",
      " |      This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\n",
      " |      \n",
      " |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      " |      `.eval()` and several similar mechanisms that may be confused with it.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  extra_repr(self) -> str\n",
      " |      Set the extra representation of the module.\n",
      " |      \n",
      " |      To print customized extra information, you should re-implement\n",
      " |      this method in your own modules. Both single-line and multi-line\n",
      " |      strings are acceptable.\n",
      " |  \n",
      " |  float(self: ~T) -> ~T\n",
      " |      Casts all floating point parameters and buffers to ``float`` datatype.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  forward = _forward_unimplemented(self, *input: Any) -> None\n",
      " |      Define the computation performed at every call.\n",
      " |      \n",
      " |      Should be overridden by all subclasses.\n",
      " |      \n",
      " |      .. note::\n",
      " |          Although the recipe for forward pass needs to be defined within\n",
      " |          this function, one should call the :class:`Module` instance afterwards\n",
      " |          instead of this since the former takes care of running the\n",
      " |          registered hooks while the latter silently ignores them.\n",
      " |  \n",
      " |  get_buffer(self, target: str) -> 'Tensor'\n",
      " |      Return the buffer given by ``target`` if it exists, otherwise throw an error.\n",
      " |      \n",
      " |      See the docstring for ``get_submodule`` for a more detailed\n",
      " |      explanation of this method's functionality as well as how to\n",
      " |      correctly specify ``target``.\n",
      " |      \n",
      " |      Args:\n",
      " |          target: The fully-qualified string name of the buffer\n",
      " |              to look for. (See ``get_submodule`` for how to specify a\n",
      " |              fully-qualified string.)\n",
      " |      \n",
      " |      Returns:\n",
      " |          torch.Tensor: The buffer referenced by ``target``\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: If the target string references an invalid\n",
      " |              path or resolves to something that is not a\n",
      " |              buffer\n",
      " |  \n",
      " |  get_extra_state(self) -> Any\n",
      " |      Return any extra state to include in the module's state_dict.\n",
      " |      \n",
      " |      Implement this and a corresponding :func:`set_extra_state` for your module\n",
      " |      if you need to store extra state. This function is called when building the\n",
      " |      module's `state_dict()`.\n",
      " |      \n",
      " |      Note that extra state should be picklable to ensure working serialization\n",
      " |      of the state_dict. We only provide provide backwards compatibility guarantees\n",
      " |      for serializing Tensors; other objects may break backwards compatibility if\n",
      " |      their serialized pickled form changes.\n",
      " |      \n",
      " |      Returns:\n",
      " |          object: Any extra state to store in the module's state_dict\n",
      " |  \n",
      " |  get_parameter(self, target: str) -> 'Parameter'\n",
      " |      Return the parameter given by ``target`` if it exists, otherwise throw an error.\n",
      " |      \n",
      " |      See the docstring for ``get_submodule`` for a more detailed\n",
      " |      explanation of this method's functionality as well as how to\n",
      " |      correctly specify ``target``.\n",
      " |      \n",
      " |      Args:\n",
      " |          target: The fully-qualified string name of the Parameter\n",
      " |              to look for. (See ``get_submodule`` for how to specify a\n",
      " |              fully-qualified string.)\n",
      " |      \n",
      " |      Returns:\n",
      " |          torch.nn.Parameter: The Parameter referenced by ``target``\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: If the target string references an invalid\n",
      " |              path or resolves to something that is not an\n",
      " |              ``nn.Parameter``\n",
      " |  \n",
      " |  get_submodule(self, target: str) -> 'Module'\n",
      " |      Return the submodule given by ``target`` if it exists, otherwise throw an error.\n",
      " |      \n",
      " |      For example, let's say you have an ``nn.Module`` ``A`` that\n",
      " |      looks like this:\n",
      " |      \n",
      " |      .. code-block:: text\n",
      " |      \n",
      " |          A(\n",
      " |              (net_b): Module(\n",
      " |                  (net_c): Module(\n",
      " |                      (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n",
      " |                  )\n",
      " |                  (linear): Linear(in_features=100, out_features=200, bias=True)\n",
      " |              )\n",
      " |          )\n",
      " |      \n",
      " |      (The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\n",
      " |      submodule ``net_b``, which itself has two submodules ``net_c``\n",
      " |      and ``linear``. ``net_c`` then has a submodule ``conv``.)\n",
      " |      \n",
      " |      To check whether or not we have the ``linear`` submodule, we\n",
      " |      would call ``get_submodule(\"net_b.linear\")``. To check whether\n",
      " |      we have the ``conv`` submodule, we would call\n",
      " |      ``get_submodule(\"net_b.net_c.conv\")``.\n",
      " |      \n",
      " |      The runtime of ``get_submodule`` is bounded by the degree\n",
      " |      of module nesting in ``target``. A query against\n",
      " |      ``named_modules`` achieves the same result, but it is O(N) in\n",
      " |      the number of transitive modules. So, for a simple check to see\n",
      " |      if some submodule exists, ``get_submodule`` should always be\n",
      " |      used.\n",
      " |      \n",
      " |      Args:\n",
      " |          target: The fully-qualified string name of the submodule\n",
      " |              to look for. (See above example for how to specify a\n",
      " |              fully-qualified string.)\n",
      " |      \n",
      " |      Returns:\n",
      " |          torch.nn.Module: The submodule referenced by ``target``\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: If the target string references an invalid\n",
      " |              path or resolves to something that is not an\n",
      " |              ``nn.Module``\n",
      " |  \n",
      " |  half(self: ~T) -> ~T\n",
      " |      Casts all floating point parameters and buffers to ``half`` datatype.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  ipu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      " |      Move all model parameters and buffers to the IPU.\n",
      " |      \n",
      " |      This also makes associated parameters and buffers different objects. So\n",
      " |      it should be called before constructing optimizer if the module will\n",
      " |      live on IPU while being optimized.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          device (int, optional): if specified, all parameters will be\n",
      " |              copied to that device\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  load_state_dict(self, state_dict: Mapping[str, Any], strict: bool = True, assign: bool = False)\n",
      " |      Copy parameters and buffers from :attr:`state_dict` into this module and its descendants.\n",
      " |      \n",
      " |      If :attr:`strict` is ``True``, then\n",
      " |      the keys of :attr:`state_dict` must exactly match the keys returned\n",
      " |      by this module's :meth:`~torch.nn.Module.state_dict` function.\n",
      " |      \n",
      " |      .. warning::\n",
      " |          If :attr:`assign` is ``True`` the optimizer must be created after\n",
      " |          the call to :attr:`load_state_dict` unless\n",
      " |          :func:`~torch.__future__.get_swap_module_params_on_conversion` is ``True``.\n",
      " |      \n",
      " |      Args:\n",
      " |          state_dict (dict): a dict containing parameters and\n",
      " |              persistent buffers.\n",
      " |          strict (bool, optional): whether to strictly enforce that the keys\n",
      " |              in :attr:`state_dict` match the keys returned by this module's\n",
      " |              :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\n",
      " |          assign (bool, optional): When ``False``, the properties of the tensors\n",
      " |              in the current module are preserved while when ``True``, the\n",
      " |              properties of the Tensors in the state dict are preserved. The only\n",
      " |              exception is the ``requires_grad`` field of :class:`~torch.nn.Parameter`s\n",
      " |              for which the value from the module is preserved.\n",
      " |              Default: ``False``\n",
      " |      \n",
      " |      Returns:\n",
      " |          ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n",
      " |              * **missing_keys** is a list of str containing any keys that are expected\n",
      " |                  by this module but missing from the provided ``state_dict``.\n",
      " |              * **unexpected_keys** is a list of str containing the keys that are not\n",
      " |                  expected by this module but present in the provided ``state_dict``.\n",
      " |      \n",
      " |      Note:\n",
      " |          If a parameter or buffer is registered as ``None`` and its corresponding key\n",
      " |          exists in :attr:`state_dict`, :meth:`load_state_dict` will raise a\n",
      " |          ``RuntimeError``.\n",
      " |  \n",
      " |  modules(self) -> Iterator[ForwardRef('Module')]\n",
      " |      Return an iterator over all modules in the network.\n",
      " |      \n",
      " |      Yields:\n",
      " |          Module: a module in the network\n",
      " |      \n",
      " |      Note:\n",
      " |          Duplicate modules are returned only once. In the following\n",
      " |          example, ``l`` will be returned only once.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> l = nn.Linear(2, 2)\n",
      " |          >>> net = nn.Sequential(l, l)\n",
      " |          >>> for idx, m in enumerate(net.modules()):\n",
      " |          ...     print(idx, '->', m)\n",
      " |      \n",
      " |          0 -> Sequential(\n",
      " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      " |          )\n",
      " |          1 -> Linear(in_features=2, out_features=2, bias=True)\n",
      " |  \n",
      " |  mtia(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      " |      Move all model parameters and buffers to the MTIA.\n",
      " |      \n",
      " |      This also makes associated parameters and buffers different objects. So\n",
      " |      it should be called before constructing optimizer if the module will\n",
      " |      live on MTIA while being optimized.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          device (int, optional): if specified, all parameters will be\n",
      " |              copied to that device\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  named_buffers(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> Iterator[Tuple[str, torch.Tensor]]\n",
      " |      Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.\n",
      " |      \n",
      " |      Args:\n",
      " |          prefix (str): prefix to prepend to all buffer names.\n",
      " |          recurse (bool, optional): if True, then yields buffers of this module\n",
      " |              and all submodules. Otherwise, yields only buffers that\n",
      " |              are direct members of this module. Defaults to True.\n",
      " |          remove_duplicate (bool, optional): whether to remove the duplicated buffers in the result. Defaults to True.\n",
      " |      \n",
      " |      Yields:\n",
      " |          (str, torch.Tensor): Tuple containing the name and buffer\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      " |          >>> for name, buf in self.named_buffers():\n",
      " |          >>>     if name in ['running_var']:\n",
      " |          >>>         print(buf.size())\n",
      " |  \n",
      " |  named_children(self) -> Iterator[Tuple[str, ForwardRef('Module')]]\n",
      " |      Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself.\n",
      " |      \n",
      " |      Yields:\n",
      " |          (str, Module): Tuple containing a name and child module\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      " |          >>> for name, module in model.named_children():\n",
      " |          >>>     if name in ['conv4', 'conv5']:\n",
      " |          >>>         print(module)\n",
      " |  \n",
      " |  named_modules(self, memo: Optional[Set[ForwardRef('Module')]] = None, prefix: str = '', remove_duplicate: bool = True)\n",
      " |      Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself.\n",
      " |      \n",
      " |      Args:\n",
      " |          memo: a memo to store the set of modules already added to the result\n",
      " |          prefix: a prefix that will be added to the name of the module\n",
      " |          remove_duplicate: whether to remove the duplicated module instances in the result\n",
      " |              or not\n",
      " |      \n",
      " |      Yields:\n",
      " |          (str, Module): Tuple of name and module\n",
      " |      \n",
      " |      Note:\n",
      " |          Duplicate modules are returned only once. In the following\n",
      " |          example, ``l`` will be returned only once.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> l = nn.Linear(2, 2)\n",
      " |          >>> net = nn.Sequential(l, l)\n",
      " |          >>> for idx, m in enumerate(net.named_modules()):\n",
      " |          ...     print(idx, '->', m)\n",
      " |      \n",
      " |          0 -> ('', Sequential(\n",
      " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      " |          ))\n",
      " |          1 -> ('0', Linear(in_features=2, out_features=2, bias=True))\n",
      " |  \n",
      " |  named_parameters(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> Iterator[Tuple[str, torch.nn.parameter.Parameter]]\n",
      " |      Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.\n",
      " |      \n",
      " |      Args:\n",
      " |          prefix (str): prefix to prepend to all parameter names.\n",
      " |          recurse (bool): if True, then yields parameters of this module\n",
      " |              and all submodules. Otherwise, yields only parameters that\n",
      " |              are direct members of this module.\n",
      " |          remove_duplicate (bool, optional): whether to remove the duplicated\n",
      " |              parameters in the result. Defaults to True.\n",
      " |      \n",
      " |      Yields:\n",
      " |          (str, Parameter): Tuple containing the name and parameter\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      " |          >>> for name, param in self.named_parameters():\n",
      " |          >>>     if name in ['bias']:\n",
      " |          >>>         print(param.size())\n",
      " |  \n",
      " |  parameters(self, recurse: bool = True) -> Iterator[torch.nn.parameter.Parameter]\n",
      " |      Return an iterator over module parameters.\n",
      " |      \n",
      " |      This is typically passed to an optimizer.\n",
      " |      \n",
      " |      Args:\n",
      " |          recurse (bool): if True, then yields parameters of this module\n",
      " |              and all submodules. Otherwise, yields only parameters that\n",
      " |              are direct members of this module.\n",
      " |      \n",
      " |      Yields:\n",
      " |          Parameter: module parameter\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      " |          >>> for param in model.parameters():\n",
      " |          >>>     print(type(param), param.size())\n",
      " |          <class 'torch.Tensor'> (20L,)\n",
      " |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      " |  \n",
      " |  register_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      " |      Register a backward hook on the module.\n",
      " |      \n",
      " |      This function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and\n",
      " |      the behavior of this function will change in future versions.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_buffer(self, name: str, tensor: Optional[torch.Tensor], persistent: bool = True) -> None\n",
      " |      Add a buffer to the module.\n",
      " |      \n",
      " |      This is typically used to register a buffer that should not to be\n",
      " |      considered a model parameter. For example, BatchNorm's ``running_mean``\n",
      " |      is not a parameter, but is part of the module's state. Buffers, by\n",
      " |      default, are persistent and will be saved alongside parameters. This\n",
      " |      behavior can be changed by setting :attr:`persistent` to ``False``. The\n",
      " |      only difference between a persistent buffer and a non-persistent buffer\n",
      " |      is that the latter will not be a part of this module's\n",
      " |      :attr:`state_dict`.\n",
      " |      \n",
      " |      Buffers can be accessed as attributes using given names.\n",
      " |      \n",
      " |      Args:\n",
      " |          name (str): name of the buffer. The buffer can be accessed\n",
      " |              from this module using the given name\n",
      " |          tensor (Tensor or None): buffer to be registered. If ``None``, then operations\n",
      " |              that run on buffers, such as :attr:`cuda`, are ignored. If ``None``,\n",
      " |              the buffer is **not** included in the module's :attr:`state_dict`.\n",
      " |          persistent (bool): whether the buffer is part of this module's\n",
      " |              :attr:`state_dict`.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      " |          >>> self.register_buffer('running_mean', torch.zeros(num_features))\n",
      " |  \n",
      " |  register_forward_hook(self, hook: Union[Callable[[~T, Tuple[Any, ...], Any], Optional[Any]], Callable[[~T, Tuple[Any, ...], Dict[str, Any], Any], Optional[Any]]], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -> torch.utils.hooks.RemovableHandle\n",
      " |      Register a forward hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time after :func:`forward` has computed an output.\n",
      " |      \n",
      " |      If ``with_kwargs`` is ``False`` or not specified, the input contains only\n",
      " |      the positional arguments given to the module. Keyword arguments won't be\n",
      " |      passed to the hooks and only to the ``forward``. The hook can modify the\n",
      " |      output. It can modify the input inplace but it will not have effect on\n",
      " |      forward since this is called after :func:`forward` is called. The hook\n",
      " |      should have the following signature::\n",
      " |      \n",
      " |          hook(module, args, output) -> None or modified output\n",
      " |      \n",
      " |      If ``with_kwargs`` is ``True``, the forward hook will be passed the\n",
      " |      ``kwargs`` given to the forward function and be expected to return the\n",
      " |      output possibly modified. The hook should have the following signature::\n",
      " |      \n",
      " |          hook(module, args, kwargs, output) -> None or modified output\n",
      " |      \n",
      " |      Args:\n",
      " |          hook (Callable): The user defined hook to be registered.\n",
      " |          prepend (bool): If ``True``, the provided ``hook`` will be fired\n",
      " |              before all existing ``forward`` hooks on this\n",
      " |              :class:`torch.nn.modules.Module`. Otherwise, the provided\n",
      " |              ``hook`` will be fired after all existing ``forward`` hooks on\n",
      " |              this :class:`torch.nn.modules.Module`. Note that global\n",
      " |              ``forward`` hooks registered with\n",
      " |              :func:`register_module_forward_hook` will fire before all hooks\n",
      " |              registered by this method.\n",
      " |              Default: ``False``\n",
      " |          with_kwargs (bool): If ``True``, the ``hook`` will be passed the\n",
      " |              kwargs given to the forward function.\n",
      " |              Default: ``False``\n",
      " |          always_call (bool): If ``True`` the ``hook`` will be run regardless of\n",
      " |              whether an exception is raised while calling the Module.\n",
      " |              Default: ``False``\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_forward_pre_hook(self, hook: Union[Callable[[~T, Tuple[Any, ...]], Optional[Any]], Callable[[~T, Tuple[Any, ...], Dict[str, Any]], Optional[Tuple[Any, Dict[str, Any]]]]], *, prepend: bool = False, with_kwargs: bool = False) -> torch.utils.hooks.RemovableHandle\n",
      " |      Register a forward pre-hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time before :func:`forward` is invoked.\n",
      " |      \n",
      " |      \n",
      " |      If ``with_kwargs`` is false or not specified, the input contains only\n",
      " |      the positional arguments given to the module. Keyword arguments won't be\n",
      " |      passed to the hooks and only to the ``forward``. The hook can modify the\n",
      " |      input. User can either return a tuple or a single modified value in the\n",
      " |      hook. We will wrap the value into a tuple if a single value is returned\n",
      " |      (unless that value is already a tuple). The hook should have the\n",
      " |      following signature::\n",
      " |      \n",
      " |          hook(module, args) -> None or modified input\n",
      " |      \n",
      " |      If ``with_kwargs`` is true, the forward pre-hook will be passed the\n",
      " |      kwargs given to the forward function. And if the hook modifies the\n",
      " |      input, both the args and kwargs should be returned. The hook should have\n",
      " |      the following signature::\n",
      " |      \n",
      " |          hook(module, args, kwargs) -> None or a tuple of modified input and kwargs\n",
      " |      \n",
      " |      Args:\n",
      " |          hook (Callable): The user defined hook to be registered.\n",
      " |          prepend (bool): If true, the provided ``hook`` will be fired before\n",
      " |              all existing ``forward_pre`` hooks on this\n",
      " |              :class:`torch.nn.modules.Module`. Otherwise, the provided\n",
      " |              ``hook`` will be fired after all existing ``forward_pre`` hooks\n",
      " |              on this :class:`torch.nn.modules.Module`. Note that global\n",
      " |              ``forward_pre`` hooks registered with\n",
      " |              :func:`register_module_forward_pre_hook` will fire before all\n",
      " |              hooks registered by this method.\n",
      " |              Default: ``False``\n",
      " |          with_kwargs (bool): If true, the ``hook`` will be passed the kwargs\n",
      " |              given to the forward function.\n",
      " |              Default: ``False``\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_full_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle\n",
      " |      Register a backward hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time the gradients with respect to a module\n",
      " |      are computed, i.e. the hook will execute if and only if the gradients with\n",
      " |      respect to module outputs are computed. The hook should have the following\n",
      " |      signature::\n",
      " |      \n",
      " |          hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n",
      " |      \n",
      " |      The :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\n",
      " |      with respect to the inputs and outputs respectively. The hook should\n",
      " |      not modify its arguments, but it can optionally return a new gradient with\n",
      " |      respect to the input that will be used in place of :attr:`grad_input` in\n",
      " |      subsequent computations. :attr:`grad_input` will only correspond to the inputs given\n",
      " |      as positional arguments and all kwarg arguments are ignored. Entries\n",
      " |      in :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor\n",
      " |      arguments.\n",
      " |      \n",
      " |      For technical reasons, when this hook is applied to a Module, its forward function will\n",
      " |      receive a view of each Tensor passed to the Module. Similarly the caller will receive a view\n",
      " |      of each Tensor returned by the Module's forward function.\n",
      " |      \n",
      " |      .. warning ::\n",
      " |          Modifying inputs or outputs inplace is not allowed when using backward hooks and\n",
      " |          will raise an error.\n",
      " |      \n",
      " |      Args:\n",
      " |          hook (Callable): The user-defined hook to be registered.\n",
      " |          prepend (bool): If true, the provided ``hook`` will be fired before\n",
      " |              all existing ``backward`` hooks on this\n",
      " |              :class:`torch.nn.modules.Module`. Otherwise, the provided\n",
      " |              ``hook`` will be fired after all existing ``backward`` hooks on\n",
      " |              this :class:`torch.nn.modules.Module`. Note that global\n",
      " |              ``backward`` hooks registered with\n",
      " |              :func:`register_module_full_backward_hook` will fire before\n",
      " |              all hooks registered by this method.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_full_backward_pre_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle\n",
      " |      Register a backward pre-hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time the gradients for the module are computed.\n",
      " |      The hook should have the following signature::\n",
      " |      \n",
      " |          hook(module, grad_output) -> tuple[Tensor] or None\n",
      " |      \n",
      " |      The :attr:`grad_output` is a tuple. The hook should\n",
      " |      not modify its arguments, but it can optionally return a new gradient with\n",
      " |      respect to the output that will be used in place of :attr:`grad_output` in\n",
      " |      subsequent computations. Entries in :attr:`grad_output` will be ``None`` for\n",
      " |      all non-Tensor arguments.\n",
      " |      \n",
      " |      For technical reasons, when this hook is applied to a Module, its forward function will\n",
      " |      receive a view of each Tensor passed to the Module. Similarly the caller will receive a view\n",
      " |      of each Tensor returned by the Module's forward function.\n",
      " |      \n",
      " |      .. warning ::\n",
      " |          Modifying inputs inplace is not allowed when using backward hooks and\n",
      " |          will raise an error.\n",
      " |      \n",
      " |      Args:\n",
      " |          hook (Callable): The user-defined hook to be registered.\n",
      " |          prepend (bool): If true, the provided ``hook`` will be fired before\n",
      " |              all existing ``backward_pre`` hooks on this\n",
      " |              :class:`torch.nn.modules.Module`. Otherwise, the provided\n",
      " |              ``hook`` will be fired after all existing ``backward_pre`` hooks\n",
      " |              on this :class:`torch.nn.modules.Module`. Note that global\n",
      " |              ``backward_pre`` hooks registered with\n",
      " |              :func:`register_module_full_backward_pre_hook` will fire before\n",
      " |              all hooks registered by this method.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_load_state_dict_post_hook(self, hook)\n",
      " |      Register a post-hook to be run after module's :meth:`~nn.Module.load_state_dict` is called.\n",
      " |      \n",
      " |      It should have the following signature::\n",
      " |          hook(module, incompatible_keys) -> None\n",
      " |      \n",
      " |      The ``module`` argument is the current module that this hook is registered\n",
      " |      on, and the ``incompatible_keys`` argument is a ``NamedTuple`` consisting\n",
      " |      of attributes ``missing_keys`` and ``unexpected_keys``. ``missing_keys``\n",
      " |      is a ``list`` of ``str`` containing the missing keys and\n",
      " |      ``unexpected_keys`` is a ``list`` of ``str`` containing the unexpected keys.\n",
      " |      \n",
      " |      The given incompatible_keys can be modified inplace if needed.\n",
      " |      \n",
      " |      Note that the checks performed when calling :func:`load_state_dict` with\n",
      " |      ``strict=True`` are affected by modifications the hook makes to\n",
      " |      ``missing_keys`` or ``unexpected_keys``, as expected. Additions to either\n",
      " |      set of keys will result in an error being thrown when ``strict=True``, and\n",
      " |      clearing out both missing and unexpected keys will avoid an error.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_load_state_dict_pre_hook(self, hook)\n",
      " |      Register a pre-hook to be run before module's :meth:`~nn.Module.load_state_dict` is called.\n",
      " |      \n",
      " |      It should have the following signature::\n",
      " |          hook(module, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs) -> None  # noqa: B950\n",
      " |      \n",
      " |      Arguments:\n",
      " |          hook (Callable): Callable hook that will be invoked before\n",
      " |              loading the state dict.\n",
      " |  \n",
      " |  register_module(self, name: str, module: Optional[ForwardRef('Module')]) -> None\n",
      " |      Alias for :func:`add_module`.\n",
      " |  \n",
      " |  register_parameter(self, name: str, param: Optional[torch.nn.parameter.Parameter]) -> None\n",
      " |      Add a parameter to the module.\n",
      " |      \n",
      " |      The parameter can be accessed as an attribute using given name.\n",
      " |      \n",
      " |      Args:\n",
      " |          name (str): name of the parameter. The parameter can be accessed\n",
      " |              from this module using the given name\n",
      " |          param (Parameter or None): parameter to be added to the module. If\n",
      " |              ``None``, then operations that run on parameters, such as :attr:`cuda`,\n",
      " |              are ignored. If ``None``, the parameter is **not** included in the\n",
      " |              module's :attr:`state_dict`.\n",
      " |  \n",
      " |  register_state_dict_post_hook(self, hook)\n",
      " |      Register a post-hook for the :meth:`~torch.nn.Module.state_dict` method.\n",
      " |      \n",
      " |      It should have the following signature::\n",
      " |          hook(module, state_dict, prefix, local_metadata) -> None\n",
      " |      \n",
      " |      The registered hooks can modify the ``state_dict`` inplace.\n",
      " |  \n",
      " |  register_state_dict_pre_hook(self, hook)\n",
      " |      Register a pre-hook for the :meth:`~torch.nn.Module.state_dict` method.\n",
      " |      \n",
      " |      It should have the following signature::\n",
      " |          hook(module, prefix, keep_vars) -> None\n",
      " |      \n",
      " |      The registered hooks can be used to perform pre-processing before the ``state_dict``\n",
      " |      call is made.\n",
      " |  \n",
      " |  requires_grad_(self: ~T, requires_grad: bool = True) -> ~T\n",
      " |      Change if autograd should record operations on parameters in this module.\n",
      " |      \n",
      " |      This method sets the parameters' :attr:`requires_grad` attributes\n",
      " |      in-place.\n",
      " |      \n",
      " |      This method is helpful for freezing part of the module for finetuning\n",
      " |      or training parts of a model individually (e.g., GAN training).\n",
      " |      \n",
      " |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      " |      `.requires_grad_()` and several similar mechanisms that may be confused with it.\n",
      " |      \n",
      " |      Args:\n",
      " |          requires_grad (bool): whether autograd should record operations on\n",
      " |                                parameters in this module. Default: ``True``.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  set_extra_state(self, state: Any) -> None\n",
      " |      Set extra state contained in the loaded `state_dict`.\n",
      " |      \n",
      " |      This function is called from :func:`load_state_dict` to handle any extra state\n",
      " |      found within the `state_dict`. Implement this function and a corresponding\n",
      " |      :func:`get_extra_state` for your module if you need to store extra state within its\n",
      " |      `state_dict`.\n",
      " |      \n",
      " |      Args:\n",
      " |          state (dict): Extra state from the `state_dict`\n",
      " |  \n",
      " |  set_submodule(self, target: str, module: 'Module') -> None\n",
      " |      Set the submodule given by ``target`` if it exists, otherwise throw an error.\n",
      " |      \n",
      " |      For example, let's say you have an ``nn.Module`` ``A`` that\n",
      " |      looks like this:\n",
      " |      \n",
      " |      .. code-block:: text\n",
      " |      \n",
      " |          A(\n",
      " |              (net_b): Module(\n",
      " |                  (net_c): Module(\n",
      " |                      (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n",
      " |                  )\n",
      " |                  (linear): Linear(in_features=100, out_features=200, bias=True)\n",
      " |              )\n",
      " |          )\n",
      " |      \n",
      " |      (The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\n",
      " |      submodule ``net_b``, which itself has two submodules ``net_c``\n",
      " |      and ``linear``. ``net_c`` then has a submodule ``conv``.)\n",
      " |      \n",
      " |      To overide the ``Conv2d`` with a new submodule ``Linear``, you\n",
      " |      would call\n",
      " |      ``set_submodule(\"net_b.net_c.conv\", nn.Linear(33, 16))``.\n",
      " |      \n",
      " |      Args:\n",
      " |          target: The fully-qualified string name of the submodule\n",
      " |              to look for. (See above example for how to specify a\n",
      " |              fully-qualified string.)\n",
      " |          module: The module to set the submodule to.\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValueError: If the target string is empty\n",
      " |          AttributeError: If the target string references an invalid\n",
      " |              path or resolves to something that is not an\n",
      " |              ``nn.Module``\n",
      " |  \n",
      " |  share_memory(self: ~T) -> ~T\n",
      " |      See :meth:`torch.Tensor.share_memory_`.\n",
      " |  \n",
      " |  state_dict(self, *args, destination=None, prefix='', keep_vars=False)\n",
      " |      Return a dictionary containing references to the whole state of the module.\n",
      " |      \n",
      " |      Both parameters and persistent buffers (e.g. running averages) are\n",
      " |      included. Keys are corresponding parameter and buffer names.\n",
      " |      Parameters and buffers set to ``None`` are not included.\n",
      " |      \n",
      " |      .. note::\n",
      " |          The returned object is a shallow copy. It contains references\n",
      " |          to the module's parameters and buffers.\n",
      " |      \n",
      " |      .. warning::\n",
      " |          Currently ``state_dict()`` also accepts positional arguments for\n",
      " |          ``destination``, ``prefix`` and ``keep_vars`` in order. However,\n",
      " |          this is being deprecated and keyword arguments will be enforced in\n",
      " |          future releases.\n",
      " |      \n",
      " |      .. warning::\n",
      " |          Please avoid the use of argument ``destination`` as it is not\n",
      " |          designed for end-users.\n",
      " |      \n",
      " |      Args:\n",
      " |          destination (dict, optional): If provided, the state of module will\n",
      " |              be updated into the dict and the same object is returned.\n",
      " |              Otherwise, an ``OrderedDict`` will be created and returned.\n",
      " |              Default: ``None``.\n",
      " |          prefix (str, optional): a prefix added to parameter and buffer\n",
      " |              names to compose the keys in state_dict. Default: ``''``.\n",
      " |          keep_vars (bool, optional): by default the :class:`~torch.Tensor` s\n",
      " |              returned in the state dict are detached from autograd. If it's\n",
      " |              set to ``True``, detaching will not be performed.\n",
      " |              Default: ``False``.\n",
      " |      \n",
      " |      Returns:\n",
      " |          dict:\n",
      " |              a dictionary containing a whole state of the module\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      " |          >>> module.state_dict().keys()\n",
      " |          ['bias', 'weight']\n",
      " |  \n",
      " |  to(self, *args, **kwargs)\n",
      " |      Move and/or cast the parameters and buffers.\n",
      " |      \n",
      " |      This can be called as\n",
      " |      \n",
      " |      .. function:: to(device=None, dtype=None, non_blocking=False)\n",
      " |         :noindex:\n",
      " |      \n",
      " |      .. function:: to(dtype, non_blocking=False)\n",
      " |         :noindex:\n",
      " |      \n",
      " |      .. function:: to(tensor, non_blocking=False)\n",
      " |         :noindex:\n",
      " |      \n",
      " |      .. function:: to(memory_format=torch.channels_last)\n",
      " |         :noindex:\n",
      " |      \n",
      " |      Its signature is similar to :meth:`torch.Tensor.to`, but only accepts\n",
      " |      floating point or complex :attr:`dtype`\\ s. In addition, this method will\n",
      " |      only cast the floating point or complex parameters and buffers to :attr:`dtype`\n",
      " |      (if given). The integral parameters and buffers will be moved\n",
      " |      :attr:`device`, if that is given, but with dtypes unchanged. When\n",
      " |      :attr:`non_blocking` is set, it tries to convert/move asynchronously\n",
      " |      with respect to the host if possible, e.g., moving CPU Tensors with\n",
      " |      pinned memory to CUDA devices.\n",
      " |      \n",
      " |      See below for examples.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Args:\n",
      " |          device (:class:`torch.device`): the desired device of the parameters\n",
      " |              and buffers in this module\n",
      " |          dtype (:class:`torch.dtype`): the desired floating point or complex dtype of\n",
      " |              the parameters and buffers in this module\n",
      " |          tensor (torch.Tensor): Tensor whose dtype and device are the desired\n",
      " |              dtype and device for all parameters and buffers in this module\n",
      " |          memory_format (:class:`torch.memory_format`): the desired memory\n",
      " |              format for 4D parameters and buffers in this module (keyword\n",
      " |              only argument)\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |      \n",
      " |      Examples::\n",
      " |      \n",
      " |          >>> # xdoctest: +IGNORE_WANT(\"non-deterministic\")\n",
      " |          >>> linear = nn.Linear(2, 2)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1913, -0.3420],\n",
      " |                  [-0.5113, -0.2325]])\n",
      " |          >>> linear.to(torch.double)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1913, -0.3420],\n",
      " |                  [-0.5113, -0.2325]], dtype=torch.float64)\n",
      " |          >>> # xdoctest: +REQUIRES(env:TORCH_DOCTEST_CUDA1)\n",
      " |          >>> gpu1 = torch.device(\"cuda:1\")\n",
      " |          >>> linear.to(gpu1, dtype=torch.half, non_blocking=True)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1914, -0.3420],\n",
      " |                  [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n",
      " |          >>> cpu = torch.device(\"cpu\")\n",
      " |          >>> linear.to(cpu)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1914, -0.3420],\n",
      " |                  [-0.5112, -0.2324]], dtype=torch.float16)\n",
      " |      \n",
      " |          >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.3741+0.j,  0.2382+0.j],\n",
      " |                  [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)\n",
      " |          >>> linear(torch.ones(3, 2, dtype=torch.cdouble))\n",
      " |          tensor([[0.6122+0.j, 0.1150+0.j],\n",
      " |                  [0.6122+0.j, 0.1150+0.j],\n",
      " |                  [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)\n",
      " |  \n",
      " |  to_empty(self: ~T, *, device: Union[int, str, torch.device, NoneType], recurse: bool = True) -> ~T\n",
      " |      Move the parameters and buffers to the specified device without copying storage.\n",
      " |      \n",
      " |      Args:\n",
      " |          device (:class:`torch.device`): The desired device of the parameters\n",
      " |              and buffers in this module.\n",
      " |          recurse (bool): Whether parameters and buffers of submodules should\n",
      " |              be recursively moved to the specified device.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  type(self: ~T, dst_type: Union[torch.dtype, str]) -> ~T\n",
      " |      Casts all parameters and buffers to :attr:`dst_type`.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Args:\n",
      " |          dst_type (type or string): the desired type\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  xpu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      " |      Move all model parameters and buffers to the XPU.\n",
      " |      \n",
      " |      This also makes associated parameters and buffers different objects. So\n",
      " |      it should be called before constructing optimizer if the module will\n",
      " |      live on XPU while being optimized.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          device (int, optional): if specified, all parameters will be\n",
      " |              copied to that device\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  zero_grad(self, set_to_none: bool = True) -> None\n",
      " |      Reset gradients of all model parameters.\n",
      " |      \n",
      " |      See similar function under :class:`torch.optim.Optimizer` for more context.\n",
      " |      \n",
      " |      Args:\n",
      " |          set_to_none (bool): instead of setting to zero, set the grads to None.\n",
      " |              See :meth:`torch.optim.Optimizer.zero_grad` for details.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  T_destination = ~T_destination\n",
      " |  \n",
      " |  call_super_init = False\n",
      " |  \n",
      " |  dump_patches = False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(YOLO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = model.predict(test_loader, conf=0.8, save=True, save_dir='runs/predict/test')\n",
    "\n",
    "test_metrics = model.calculate_metrics(test_predictions, test_loader)\n",
    "print(\"Test Metrics:\", test_metrics)\n",
    "\n",
    "model.plot_predictions(test_predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
